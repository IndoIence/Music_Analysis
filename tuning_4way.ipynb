{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tp/miniconda3/envs/mgr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-28 07:51:42.413123: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-28 07:51:42.439211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-28 07:51:42.856325: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 1049/1049 [00:04<00:00, 252.24it/s].58it/s]\n",
      "sorting artists by lyrics length: 1049it [00:04, 251.89it/s]\n",
      "100%|██████████| 1049/1049 [00:05<00:00, 199.13it/s].75it/s]\n",
      "sorting artists by lyrics length: 1049it [00:05, 198.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from utils import get_artist, get_biggest_arts\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from bert_lIghtning import songs_from_artists\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.cuda import empty_cache\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "MODEL_NAME = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "top10 = get_biggest_arts(10)\n",
    "top30 = get_biggest_arts(30)\n",
    "\n",
    "modes = [\"solo\", \"features\"]\n",
    "song_limits_10 = [110, 180]\n",
    "song_limits_30 = [190,300]\n",
    "arts_lists = [top10, top30]\n",
    "combined = [(top30,\"solo\",110), (top30,\"features\",180)] + [(top10,\"solo\", 190), (top10,\"features\",300)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/30_solo_110 6117 1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkatnak56\u001b[0m (\u001b[33mfirst_throw\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tp/MGR/from_gpu_server/Music_Analysis/wandb/run-20240628_075200-pqqmko0x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/first_throw/huggingface/runs/pqqmko0x' target=\"_blank\">models/30_solo_110</a></strong> to <a href='https://wandb.ai/first_throw/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/first_throw/huggingface' target=\"_blank\">https://wandb.ai/first_throw/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/first_throw/huggingface/runs/pqqmko0x' target=\"_blank\">https://wandb.ai/first_throw/huggingface/runs/pqqmko0x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2298' max='2298' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2298/2298 14:07, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.864948</td>\n",
       "      <td>0.211111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.157900</td>\n",
       "      <td>2.446953</td>\n",
       "      <td>0.338889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.429100</td>\n",
       "      <td>2.144733</td>\n",
       "      <td>0.404630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.910800</td>\n",
       "      <td>1.897428</td>\n",
       "      <td>0.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.910800</td>\n",
       "      <td>1.831291</td>\n",
       "      <td>0.489815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.491700</td>\n",
       "      <td>1.749066</td>\n",
       "      <td>0.520370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/30_features_180 10354 1828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3888' max='3888' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3888/3888 23:18, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.193000</td>\n",
       "      <td>2.721918</td>\n",
       "      <td>0.236324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.653000</td>\n",
       "      <td>2.274861</td>\n",
       "      <td>0.370897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.259700</td>\n",
       "      <td>1.983755</td>\n",
       "      <td>0.432713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.548300</td>\n",
       "      <td>1.825826</td>\n",
       "      <td>0.486324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.275200</td>\n",
       "      <td>1.702107</td>\n",
       "      <td>0.517505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.106800</td>\n",
       "      <td>1.677769</td>\n",
       "      <td>0.522429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/10_solo_190 3264 577\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1224/1224 07:26, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.565342</td>\n",
       "      <td>0.462738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.192296</td>\n",
       "      <td>0.599653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.562800</td>\n",
       "      <td>0.939511</td>\n",
       "      <td>0.679376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.562800</td>\n",
       "      <td>0.748583</td>\n",
       "      <td>0.750433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.707688</td>\n",
       "      <td>0.759099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.669406</td>\n",
       "      <td>0.774697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/10_features_300 5602 989\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2106' max='2106' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2106/2106 12:43, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.485731</td>\n",
       "      <td>0.480283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.765600</td>\n",
       "      <td>1.155149</td>\n",
       "      <td>0.587462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.997400</td>\n",
       "      <td>0.983166</td>\n",
       "      <td>0.656218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.997400</td>\n",
       "      <td>0.834856</td>\n",
       "      <td>0.707786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.571200</td>\n",
       "      <td>0.864143</td>\n",
       "      <td>0.696663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.783090</td>\n",
       "      <td>0.739130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for arts_list, mode, song_limit in combined:\n",
    "    label2id = {label: i for i, label in enumerate((a.name_sanitized for a in arts_list))}\n",
    "    data = songs_from_artists(arts_list, tokenizer, label2id=label2id, mode=mode, song_limit=song_limit)\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    df = pd.DataFrame(data)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "    train_df_dict = train_df.to_dict(orient='records')\n",
    "    test_df_dict = test_df.to_dict(orient='records')\n",
    "    test_dataset, train_dataset = Dataset.from_list(test_df_dict), Dataset.from_list(train_df_dict)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(label2id.keys()), id2label=id2label, label2id=label2id\n",
    "        )\n",
    "    output_dir = f\"models/{len(arts_list)}_{mode}_{song_limit}\"\n",
    "    print(output_dir, len(train_dataset), len(test_dataset))\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=6,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        # remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(output_dir)\n",
    "    del model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=10, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = F.nll_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='177' max='1755' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 177/1755 00:59 < 08:53, 2.96 it/s, Epoch 0.50/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# push_to_hub=True,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgr/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgr/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mgr/lib/python3.10/site-packages/transformers/trainer.py:3241\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m-> 3241\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mgr/lib/python3.10/site-packages/torch/cuda/memory.py:162\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    # push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    # tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "gc.collect()\n",
    "empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'label', '__index_level_0__'],\n",
       "    num_rows: 1676\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'label', '__index_level_0__'],\n",
       "    num_rows: 6702\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1269 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "input_ids, attention_masks = transform_text(example_song_lyrics[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 512]), torch.Size([3, 512]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#hot16challenge\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Wyje Wyje Bane\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Rainman\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Michael Kors\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "#CTZK\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Wunder-Baum\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Ostatnia Noc\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Pażałsta\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Biełyje Nosy\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "T-Killa\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Forever Ja\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Brodaggacio\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "69 Ziomeczków\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "#COHF\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Kot Gigant\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Tough Love\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "CMRT\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Feat. (+ Introdukcja)\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Drin za drinem\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Moja Natura\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Keptn’\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Szpanpan\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "DLS\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Czyste Szpanerstwo\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Dziup L.A.\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Streetwear\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Nie Banglasz\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Jeeebać Łaków Remix\n",
      "torch.Size([10, 512]) torch.Size([10, 512])\n",
      "Vanillalalahajs\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Big Poppa\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "John Rambo\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Martwe Ziomki\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Axamit\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Łatwopalność\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Bezgunaman\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Tak Nam Dobrze\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Iza Luiza\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "J23\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Rezzi (Lata Dans)\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Było Warto\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Gimb Money\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Polećmy Razem\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "To Coś\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Ja mam to co ty\n",
      "torch.Size([5, 512]) torch.Size([5, 512])\n",
      "Jupiter\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Dzisiaj Tak\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Na Pierwszej Linii\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Hoespicjum\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Melo Inferno\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Cafe O’Belga\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Spróbuj\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Hot18Banglasz\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Senymenalnie\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Wieczór Kawalerski Pt. 1\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Żelipapą\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Tłek\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Ryyyj\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Vanilla Ice\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Tylko Tyle\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Cztery Benze\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Ej ziomek Remix\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Gangin’\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Stadnina\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Noji?\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Pięć Remix\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Bednius\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Boatever\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "100k Na Insta\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Kiedy Keptn\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Dyskretny Chłód 2\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Tederminacja\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Murrrda\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Mirafiori\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Najaraj Się Marią\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Allinka\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Pump Air Nikiel\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Polonez Trapez\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "One Star\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Słek Posypany Remix\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Wydajeje\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Ola z Na Wspólnej\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Guczi Sruczi Look\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "FCMT\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Wolę się nastukać\n",
      "torch.Size([7, 512]) torch.Size([7, 512])\n",
      "Kasa\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Tanktop\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Kara’van\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "22\" AC Schnitzer\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Klaser\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Ten Bit Jak Mobb Deep (199X)\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Fame Lover\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Jeeebać Łaków\n",
      "torch.Size([4, 512]) torch.Size([4, 512])\n",
      "Moniuszko Flow\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Kurort Rolson\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "P.N.K.Ś.K.J.Z.I.N.Z.S.S.B.C.N.T.Z.I.J.M.Z.T.P.B.S.C\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Keptn’ Jack\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Autowpierdol\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Hamuj Piętą\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n",
      "Tańcuj\n",
      "torch.Size([3, 512]) torch.Size([3, 512])\n",
      "Przez Feeejm\n",
      "torch.Size([2, 512]) torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "for song in get_artist(\"Tede\").songs[:100]:\n",
    "    print(song.title)\n",
    "    input_ids, attention_masks = transform_text(song.get_clean_song_lyrics(), tokenizer)\n",
    "    print(input_ids.shape, attention_masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_biggest_arts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_arts = get_biggest_arts(30)\n",
    "songs = [song for art in top30_arts for song in art.songs[:200] if song.get_clean_song_lyrics() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = chunk_text(example_song_lyrics[0], tokenizer)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing == tokenizer.decode(tokenizer.encode(testing, add_special_tokens=False, truncation=False, return_tensors='pt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1001,  1001, 12098,  6200,  6583,  2480,  4213, 24098,  2666,  8962,\n",
      "          6633, 17491,  3217,  3676, 27838,  9761,  5004,  1010,  2000,  6448,\n",
      "          4355,  2401,  1039,  4143,  6342, 14855,  2243, 29250,  2480,  2532,\n",
      "          1062, 13476,  6200,  2278,  1012,  1012,  1012,  1012, 13970, 14756,\n",
      "          7367, 11968,  2063,  5207,  5004, 21469,  2050,  1055,  2480,  9739,\n",
      "          2226, 12849, 23344,  2102,  8034,  2022,  2480,  2933,  2226,  8945,\n",
      "          5003,  2213, 24185, 19666,  2100,  5353, 14855,  2243,  2793,  2050,\n",
      "         17235,  1052, 22123,  6305,  6583, 12170, 11283,  1039,  9096, 24185,\n",
      "          2094,  3489,  1039,  9096,  1105, 17994,  2063,  1029,  2000,  2026,\n",
      "         24185, 13728,  2100,  1105, 17994,  2063,  1105, 17994,  2050, 14768,\n",
      "          1010,  5939,  1010, 22064,  3501,  1055, 18818, 17994,  6305,  6187,\n",
      "         18818,  2063,  1059, 27006,  7033,  2617,  6776,  2000,  5003,  2213,\n",
      "         27006,  2072,  2373, 27838,  3520,  2011,  2213, 10975, 24506, 28534,\n",
      "         27966,  2050, 18818, 14163,  9096,  3489, 12849, 12273,  9096,  9033,\n",
      "          2063,  5353,  1010, 29250, 23749,  2050,  5939,  2094, 14272,  2078,\n",
      "          1059,  2100,  3501,  2094, 14272,  1010, 27838,  6583, 13113,  2522,\n",
      "          2015,  9748, 10624,  2860,  1062,  2100, 23402,  2000,  1062,  2100,\n",
      "         23402,  1012, 10958,  2480,  1055,  9096,  9818,  2666,  3501, 10958,\n",
      "          2480, 24185, 19666,  2666,  3501, 11867, 16366, 22895,  2666, 13433,\n",
      "          8569,  3900,  2213,  9033,  2063,  6519,  2050, 13433,  9358,  6824,\n",
      "          2000,  3520,  2080, 10975,  9096,  3501,  2094, 14272,  1010,  1039,\n",
      "          4143,  2015,  4830,  3501,  2079, 16686,  2050,  2771,  1011,  2149,\n",
      "          6873,  2912,  3900,  2213, 22564, 27544, 14272,  2000,  1062, 18994,\n",
      "          5937, 19817,  9096,  2213,  9033,  2063,  1010, 12403, 17130,  1062,\n",
      "         13970,  8737, 10278,  2072,  2079,  5631,  1051, 18927,  3771,  9305,\n",
      "          2050,  2771,  1010,  3393,  3401, 17214, 13433,  2000, 24401,  3630,\n",
      "          1042, 11439, 15333, 24700,  6305, 24401,  3630,  1042, 11439,  1010,\n",
      "         24401,  3630, 15333, 24700,  6305,  1042, 11439,  2000, 15333,  3367,\n",
      "          7367,  2094,  3630, 27178,  2080,  3393,  3401, 15333, 24700,  6305,\n",
      "         24401,  3630,  1042, 11439,  1010, 24401,  3630,  1042, 11439, 15333,\n",
      "         24700,  6305, 24401,  3630,  1042, 11439, 15333, 24700,  6305,  1010,\n",
      "          1051,  1039,  4143,  2015,  1052, 18818,  6038,  2666,  1010,  1052,\n",
      "         18818,  6038,  2666,  1037,  5939,  1059,  2100,  6460,  1059,  2100,\n",
      "          6460, 25163, 16137,  2480,  1059,  2100,  6460,  1059,  2100,  6460,\n",
      "         25163, 16137,  2480,  1059,  2000, 27838,  1052, 18818,  6038,  2666,\n",
      "          1039,  4143,  2015,  1039,  4143,  2015,  1052, 18818,  6038,  2666,\n",
      "          1010,  1052, 18818,  6038,  2666,  1037,  5939,  1059,  2100,  6460,\n",
      "          1059,  2100,  6460, 25163, 16137,  2480,  1059,  2100,  6460,  1059,\n",
      "          2100,  6460, 25163, 16137,  2480,  1059,  2100,  6460, 27543, 16137,\n",
      "          2480,  1039,  4143,  2015,  1052, 18818,  6038,  2666,  1010,  1052,\n",
      "         18818,  6038,  2666,  1037,  5939,  1059,  2100,  6460,  1059,  2100,\n",
      "          6460, 25163, 16137,  2480,  1059,  2100,  6460,  1059,  2100,  6460,\n",
      "         25163, 16137,  2480,  1059,  2000, 27838,  1052, 18818,  6038,  2666,\n",
      "          1039,  4143,  2015,  1039,  4143,  2015,  1052, 18818,  6038,  2666,\n",
      "          1010,  1052, 18818,  6038,  2666,  1037,  5939,  1059,  2100,  6460,\n",
      "          1059,  2100,  6460, 25163, 16137,  2480,  1059,  2100,  6460,  1059,\n",
      "          2100,  6460, 25163, 16137,  2480,  1059,  2100,  6460, 27543, 16137,\n",
      "          2480]])\n",
      "# # arnia nazwa mnie potem podroba ze stanow, to kwestia czasu jak zaczna zwalniac.... kupie se pare jordanow dla szpanu kompletnie bez planu bo mam wolny weekend jak beda nas pytac na bibie czy wodke czy łyche? to my wolmy łyche łycha sour, ty, nwj słychac całe w takich momentach to mam taki power ze sam bym przekrzyczał muzyke konczy sie weekend, zaczyna tydzien wyjdzie, ze nagram cos bangerow zycie to zycie. raz szybciej raz wolniej spokojnie pobujam sie fura po centrum to samo przyjdzie, czas daj do maja mi - uspokajam ich narazie to ziomek trzym sie, spadam z kumplami do miami odpierdala mi, lece tam po to jedno foto jebnac jedno foto, jedno jebnac foto to jest sedno oto lece jebnac jedno foto, jedno foto jebnac jedno foto jebnac, o czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz w to ze płynie czas czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz wyjebane masz czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz w to ze płynie czas czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz wyjebane masz\n",
      "##arnia nazwa mnie potem podroba ze stanow, to kwestia czasu jak zaczna zwalniac.... kupie se pare jordanow dla szpanu kompletnie bez planu bo mam wolny weekend jak beda nas pytac na bibie czy wodke czy łyche? to my wolmy łyche łycha sour, ty, nwj słychac całe w takich momentach to mam taki power ze sam bym przekrzyczał muzyke konczy sie weekend, zaczyna tydzien wyjdzie, ze nagram cos bangerow zycie to zycie. raz szybciej raz wolniej spokojnie pobujam sie fura po centrum to samo przyjdzie, czas daj do maja mi - uspokajam ich narazie to ziomek trzym sie, spadam z kumplami do miami odpierdala mi, lece tam po to jedno foto jebnac jedno foto, jedno jebnac foto to jest sedno oto lece jebnac jedno foto, jedno foto jebnac jedno foto jebnac, o czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz w to ze płynie czas czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz wyjebane masz czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz w to ze płynie czas czas płynie, płynie a ty wyje wyje bane masz wyje wyje bane masz wyjebane masz\n"
     ]
    }
   ],
   "source": [
    "testing_encoded = tokenizer.encode(testing, add_special_tokens=False, truncation=False, return_tensors='pt')\n",
    "testing_decoded = tokenizer.decode(testing_encoded[0])\n",
    "print(testing_encoded)\n",
    "print(testing_decoded)\n",
    "print(testing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
